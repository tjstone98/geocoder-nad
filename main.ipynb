{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fe8e763a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, zipfile, re, os, fiona\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "53112158",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_location = r'../data_download'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bfb4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get('https://disasters.geoplatform.gov/USA_Structures')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351ebef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = re.findall(pattern=r'USA_Structures/\\w+\\+*\\w*/Deliverable........\\S\\S.zip', string=str(response._content))\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbae6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_extract_zip(url, extract_to=r'.', filename=None):\n",
    "    if filename is None:\n",
    "        filename = url.split('/')[-1] # retrieve final portion of URL for file name\n",
    "    \n",
    "    with requests.get(url, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "\n",
    "        with open(filename, \"wb\") as f:\n",
    "            for chunk in r.iter_content(chunk_size=1024):\n",
    "                f.write(chunk)\n",
    "    \n",
    "    with zipfile.ZipFile(filename, 'r') as z:\n",
    "        z.extractall(path=extract_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6512f5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    print(f'Processing {file}...')\n",
    "    download_and_extract_zip(\n",
    "        url=f'https://fema-femadata.s3.amazonaws.com/Partners/ORNL/{file}',\n",
    "        extract_to=extract_location\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0370d296",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdbs = []\n",
    "\n",
    "for root, dir, file in os.walk(extract_location):\n",
    "    if len(dir) > 0:\n",
    "        for i in list(range(0, len(dir))):\n",
    "            if dir[i][-4:] == '.gdb':\n",
    "                gdbs.append(fr'{root}/{dir[i]}')\n",
    "print(gdbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c8a3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.GeoDataFrame()\n",
    "\n",
    "for gdb in gdbs:\n",
    "    layers = fiona.listlayers(gdb)\n",
    "\n",
    "    for layer in layers:\n",
    "        print(f'Reading {layer} in {gdb}...')\n",
    "        gdf = gpd.read_file(filename=gdb, driver='OpenFileGDB', layer=layer)\n",
    "        print(f'  > {layer} has {len(gdf.index)} total rows.')\n",
    "\n",
    "        gdf = gdf.dropna(subset=['PROP_ADDR', 'PROP_CITY']).reset_index(drop=True)\n",
    "        print(f' > {layer} has {len(gdf.index)} rows with address information.')\n",
    "\n",
    "        gdf.to_parquet(path=fr'./processing/{layer}.parquet', index=False)\n",
    "        \"\"\"\n",
    "        rows_to_add = len(temp.index)\n",
    "        expected_rows = len(gdf.index) + len(temp.index)\n",
    "\n",
    "        gdf = pd.concat([gdf, temp], ignore_index=True)\n",
    "        print(f'   > Full GDF now has {len(gdf.index)} total rows.')\n",
    "\n",
    "        if len(gdf.index) != expected_rows:\n",
    "            #raise ValueException\n",
    "            print(f'WARNING: EXPECTED ROW MISMATCH. Expected {expected_rows} rows, but returned {len(gdf.index)}.')\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "18ae89ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging AK_Structures.parquet...\n",
      "Merging AL_Structures.parquet...\n",
      "Merging AR_Structures.parquet...\n",
      "Merging AS_Structures.parquet...\n",
      "Merging AZ_Structures.parquet...\n",
      "Merging CA_Structures.parquet...\n",
      "Merging CO_Structures.parquet...\n",
      "Merging CT_Structures.parquet...\n",
      "Merging DE_Structures.parquet...\n",
      "Merging FL_Structures.parquet...\n",
      "Merging GA_Structures.parquet...\n",
      "Merging GU_Structures.parquet...\n",
      "Merging HI_Structures.parquet...\n",
      "Merging IA_Structures.parquet...\n",
      "Merging ID_Structures.parquet...\n",
      "Merging IL_Structures.parquet...\n",
      "Merging IN_Structures.parquet...\n",
      "Merging KS_Structures.parquet...\n",
      "Merging KY_Structures.parquet...\n",
      "Merging LA_Structures.parquet...\n",
      "Merging MA_Structures.parquet...\n",
      "Merging MD_Structures.parquet...\n",
      "Merging ME_Structures.parquet...\n",
      "Merging MI_Structures.parquet...\n",
      "Merging MN_Structures.parquet...\n",
      "Merging MO_Structures.parquet...\n",
      "Merging MS_Structures.parquet...\n",
      "Merging MT_Structures.parquet...\n",
      "Merging NC_Structures.parquet...\n",
      "Merging ND_Structures.parquet...\n",
      "Merging NE_Structures.parquet...\n",
      "Merging NH_Structures.parquet...\n",
      "Merging NJ_Structures.parquet...\n",
      "Merging NM_Structures.parquet...\n",
      "Merging NV_Structures.parquet...\n",
      "Merging NY_Structures.parquet...\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 950. MiB for an array with shape (3, 41509844) and data type object",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m parq \u001b[38;5;129;01min\u001b[39;00m parqs:\n\u001b[32m      7\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mMerging \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparq\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     gdf = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mgdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mfr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m./processing/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mparq\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m gdf.to_parquet(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m./outputs/merged_20251025.parquet\u001b[39m\u001b[33m'\u001b[39m, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\PersonalProjects\\Geocoder - FEMA\\.venv\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:395\u001b[39m, in \u001b[36mconcat\u001b[39m\u001b[34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[39m\n\u001b[32m    380\u001b[39m     copy = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    382\u001b[39m op = _Concatenator(\n\u001b[32m    383\u001b[39m     objs,\n\u001b[32m    384\u001b[39m     axis=axis,\n\u001b[32m   (...)\u001b[39m\u001b[32m    392\u001b[39m     sort=sort,\n\u001b[32m    393\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m395\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\PersonalProjects\\Geocoder - FEMA\\.venv\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:684\u001b[39m, in \u001b[36m_Concatenator.get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    680\u001b[39m             indexers[ax] = obj_labels.get_indexer(new_labels)\n\u001b[32m    682\u001b[39m     mgrs_indexers.append((obj._mgr, indexers))\n\u001b[32m--> \u001b[39m\u001b[32m684\u001b[39m new_data = \u001b[43mconcatenate_managers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    685\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmgrs_indexers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnew_axes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcat_axis\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbm_axis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\n\u001b[32m    686\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    687\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.copy \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_copy_on_write():\n\u001b[32m    688\u001b[39m     new_data._consolidate_inplace()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\PersonalProjects\\Geocoder - FEMA\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\concat.py:177\u001b[39m, in \u001b[36mconcatenate_managers\u001b[39m\u001b[34m(mgrs_indexers, axes, concat_axis, copy)\u001b[39m\n\u001b[32m    167\u001b[39m vals = [ju.block.values \u001b[38;5;28;01mfor\u001b[39;00m ju \u001b[38;5;129;01min\u001b[39;00m join_units]\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m blk.is_extension:\n\u001b[32m    170\u001b[39m     \u001b[38;5;66;03m# _is_uniform_join_units ensures a single dtype, so\u001b[39;00m\n\u001b[32m    171\u001b[39m     \u001b[38;5;66;03m#  we can use np.concatenate, which is more performant\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    175\u001b[39m     \u001b[38;5;66;03m# expected \"Union[_SupportsArray[dtype[Any]],\u001b[39;00m\n\u001b[32m    176\u001b[39m     \u001b[38;5;66;03m# _NestedSequence[_SupportsArray[dtype[Any]]]]\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     values = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m is_1d_only_ea_dtype(blk.dtype):\n\u001b[32m    179\u001b[39m     \u001b[38;5;66;03m# TODO(EA2D): special-casing not needed with 2D EAs\u001b[39;00m\n\u001b[32m    180\u001b[39m     values = concat_compat(vals, axis=\u001b[32m0\u001b[39m, ea_compat_axis=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mMemoryError\u001b[39m: Unable to allocate 950. MiB for an array with shape (3, 41509844) and data type object"
     ]
    }
   ],
   "source": [
    "parqs = [dir for dir in os.listdir(r'./processing') if dir.endswith('.parquet')]\n",
    "\n",
    "del gdf\n",
    "gdf = gpd.GeoDataFrame()\n",
    "\n",
    "for parq in parqs:\n",
    "    print(f'Merging {parq}...')\n",
    "    gdf = pd.concat(objs=[gdf, gpd.read_parquet(fr'./processing/{parq}')], ignore_index=True)\n",
    "gdf.to_parquet(r'./outputs/merged_20251025.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51e3c7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geocoder-fema",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
